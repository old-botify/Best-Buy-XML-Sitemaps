This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-13T20:31:46.837Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
.gitignore
botify-combine.py
lookup.py
README.md
requirements.txt

================================================================
Repository Files
================================================================

================
File: .gitignore
================
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
env/
ENV/
.env
.venv
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
.project
.pydevproject
.settings

# Project specific
output/
*.csv
!requirements.txt

# OS specific
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Logs
*.log

botify_export/
output/

================
File: botify-combine.py
================
import os
import pandas as pd
from pathlib import Path

def combine_csvs(folder_path):
    # Convert folder path to Path object
    folder = Path(folder_path)
    
    # List to store all dataframes
    dfs = []
    
    # Iterate through all CSV files in the folder
    for csv_file in folder.glob('*.csv'):
        try:
            # Read the file content
            with open(csv_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # Remove 'sep=,' if it exists at the start
            if lines and lines[0].strip() == 'sep=,':
                lines = lines[1:]
            
            # Write cleaned content to a temporary string
            from io import StringIO
            cleaned_content = StringIO(''.join(lines))
            
            # Read the cleaned CSV content
            df = pd.read_csv(cleaned_content)
            
            # Add source file column for tracking
            df['source_file'] = csv_file.name
            
            dfs.append(df)
            print(f"Successfully processed: {csv_file.name}")
            
        except Exception as e:
            print(f"Error processing {csv_file.name}: {str(e)}")
    
    if not dfs:
        raise ValueError("No CSV files were successfully processed")
    
    # Combine all dataframes
    combined_df = pd.concat(dfs, ignore_index=True)
    
    # Remove duplicates based on all columns except the source_file
    columns_for_dedup = [col for col in combined_df.columns if col != 'source_file']
    combined_df = combined_df.drop_duplicates(subset=columns_for_dedup, keep='first')
    
    return combined_df

def main():
    # Folder path containing the CSV files
    folder_path = "botify_export"
    
    try:
        # Combine CSVs
        print(f"\nProcessing CSV files in folder: {folder_path}")
        combined_df = combine_csvs(folder_path)
        
        # Generate output filename
        output_file = "combined_botify_export.csv"
        
        # Save the combined CSV
        combined_df.to_csv(output_file, index=False)
        
        # Print summary statistics
        print(f"\nSummary:")
        print(f"Total rows in combined file: {len(combined_df)}")
        print(f"Number of unique URLs: {combined_df['Full URL'].nunique()}")
        print(f"Source files processed: {combined_df['source_file'].nunique()}")
        print(f"\nOutput saved to: {output_file}")
        
    except Exception as e:
        print(f"Error: {str(e)}")

if __name__ == "__main__":
    main()

================
File: lookup.py
================
import pandas as pd
from tqdm import tqdm
import os

def process_urls_and_sitemaps(botify_file, urls_file, output_dir='./output'):
    """
    Process URLs and sitemaps from two CSV files, joining datasets and analyzing indexability
    
    Args:
        botify_file (str): Path to the Botify CSV file with status codes and indexability
        urls_file (str): Path to the URLs file containing XML sitemap data
        output_dir (str): Directory for output files
    """
    print("Loading data files...")
    
    # Read the CSVs with progress bars
    with tqdm(total=2, desc="Reading CSV files") as pbar:
        botify_df = pd.read_csv(botify_file)
        pbar.update(1)
        sitemap_df = pd.read_csv(urls_file)
        pbar.update(1)
    
    print("\nProcessing URLs and generating reports...")
    
    # Create a mapping of URL to source sitemaps (handling multiple sitemaps per URL)
    with tqdm(total=1, desc="Creating sitemap mapping") as pbar:
        sitemap_mapping = sitemap_df.groupby('URL')['Source Sitemap'].agg(lambda x: ','.join(x.unique())).reset_index()
        sitemap_mapping.columns = ['Full URL', 'Source Sitemaps']
        pbar.update(1)
    
    # Merge Botify data with sitemap data
    with tqdm(total=1, desc="Merging datasets") as pbar:
        # Left join to keep all Botify URLs and add sitemap information where available
        merged_df = pd.merge(botify_df, sitemap_mapping, on='Full URL', how='left')
        pbar.update(1)
    
    # Analyze URLs and create reports
    with tqdm(total=4, desc="Analyzing URLs") as pbar:
        # URLs in sitemaps that are non-indexable (should be removed)
        sitemap_urls_to_remove = merged_df[
            (merged_df['Source Sitemaps'].notna()) & 
            (merged_df['Non-Indexable Main Reason'].notna())
        ].copy()
        pbar.update(1)
        
        # URLs in sitemaps that are indexable (good to keep)
        sitemap_urls_valid = merged_df[
            (merged_df['Source Sitemaps'].notna()) & 
            (merged_df['Non-Indexable Main Reason'].isna())
        ].copy()
        pbar.update(1)
        
        # URLs not in sitemaps but are indexable (potential additions)
        missing_indexable_urls = merged_df[
            (merged_df['Source Sitemaps'].isna()) & 
            (merged_df['Non-Indexable Main Reason'].isna())
        ].copy()
        pbar.update(1)
        
        # Create pagetype analysis
        pagetype_analysis = merged_df.groupby(['pagetype', 'Non-Indexable Main Reason']).size().reset_index(name='count')
        pagetype_analysis = pagetype_analysis.sort_values(['pagetype', 'count'], ascending=[True, False])
        pbar.update(1)
    
    # Save outputs
    print("\nSaving output files...")
    os.makedirs(output_dir, exist_ok=True)
    
    with tqdm(total=6, desc="Writing files") as pbar:
        # Save complete dataset
        merged_df.to_csv(f'{output_dir}/complete_dataset.csv', index=False)
        pbar.update(1)
        
        # Save URLs to remove from sitemaps
        sitemap_urls_to_remove.to_csv(f'{output_dir}/urls_to_remove_from_sitemap.csv', index=False)
        pbar.update(1)
        
        # Save valid sitemap URLs
        sitemap_urls_valid.to_csv(f'{output_dir}/valid_sitemap_urls.csv', index=False)
        pbar.update(1)
        
        # Save potentially missing URLs
        missing_indexable_urls.to_csv(f'{output_dir}/missing_indexable_urls.csv', index=False)
        pbar.update(1)
        
        # Save pagetype analysis
        pagetype_analysis.to_csv(f'{output_dir}/pagetype_analysis.csv', index=False)
        pbar.update(1)
        
        # Save sitemap-specific analysis
        sitemap_analysis = merged_df[merged_df['Source Sitemaps'].notna()].groupby('Source Sitemaps')[['Non-Indexable Main Reason']].value_counts().reset_index(name='count')
        sitemap_analysis.to_csv(f'{output_dir}/sitemap_analysis.csv', index=False)
        pbar.update(1)
    
    # Print summary statistics
    print("\nProcessing complete! Summary:")
    print(f"Total URLs in Botify export: {len(botify_df)}")
    print(f"Total URLs in XML sitemaps: {len(sitemap_df)}")
    print(f"URLs in sitemaps that should be removed (non-indexable): {len(sitemap_urls_to_remove)}")
    print(f"Valid URLs in sitemaps: {len(sitemap_urls_valid)}")
    print(f"Indexable URLs not in sitemaps: {len(missing_indexable_urls)}")
    
    print("\nPagetype Analysis Preview:")
    pd.set_option('display.max_rows', 20)
    print(pagetype_analysis.head(10).to_string(index=False))
    
    print("\nOutput files have been saved to the 'output' directory:")
    print("- complete_dataset.csv: Full joined dataset with all URLs and their properties")
    print("- urls_to_remove_from_sitemap.csv: URLs currently in sitemaps that should be removed")
    print("- valid_sitemap_urls.csv: URLs correctly included in sitemaps")
    print("- missing_indexable_urls.csv: Indexable URLs that could be added to sitemaps")
    print("- pagetype_analysis.csv: Analysis of indexability by pagetype")
    print("- sitemap_analysis.csv: Analysis of issues by specific sitemap")

if __name__ == "__main__":
    process_urls_and_sitemaps(
        'combined_botify_export.csv',
        'all_extracted_urls.csv',
        './output'
    )

================
File: README.md
================
# Sitemap URL Analysis Tool

## ⚠️ Known Issues
- **complete_dataset.csv**: Currently only contains non-indexable URLs. Needs to be updated to include all URLs
- **urls_to_remove_from_sitemap.csv**: Currently showing incorrect data (showing indexable URLs instead of non-indexable). Needs to be fixed
- **valid_sitemap_urls.csv**: Output appears to be broken/incorrect
- **missing_indexable_urls.csv**: Output appears to be broken/incorrect

## Overview
This tool analyzes URLs from Botify exports and XML sitemaps to identify indexability issues and provide QA insights. It helps identify URLs that should be removed from sitemaps and provides analysis by pagetype.

## Prerequisites
- Python 3.8+
- pandas
- tqdm

## Installation
1. Clone this repository
2. Install required packages:
```bash
pip install -r requirements.txt
```

## Project Structure
```
sitemap-analysis/
├── README.md
├── .gitignore
├── requirements.txt
├── process_urls.py
└── output/
    ├── complete_dataset.csv
    ├── urls_to_remove_from_sitemap.csv
    ├── valid_sitemap_urls.csv
    ├── missing_indexable_urls.csv
    ├── pagetype_analysis.csv
    └── sitemap_analysis.csv
```

## Input Files Required
1. **combined_botify_export.csv**: Full Botify export containing:
   - Full URL
   - pagetype
   - Non-Indexable Main Reason
   - Other Botify metrics

2. **all_extracted_urls.csv**: XML sitemap data containing:
   - URL
   - Source Sitemap

## Usage
```bash
python process_urls.py
```

## Output Files
1. **complete_dataset.csv**
   - Combined dataset with Botify data and sitemap information
   - ⚠️ Currently only contains non-indexable URLs

2. **urls_to_remove_from_sitemap.csv**
   - URLs that should be removed from sitemaps (non-indexable)
   - ⚠️ Currently showing incorrect data

3. **valid_sitemap_urls.csv**
   - URLs correctly included in sitemaps
   - ⚠️ Currently broken/incorrect

4. **missing_indexable_urls.csv**
   - Indexable URLs not currently in sitemaps
   - ⚠️ Currently broken/incorrect

5. **pagetype_analysis.csv**
   - Analysis of indexability by pagetype

6. **sitemap_analysis.csv**
   - Analysis of issues by specific sitemap

## Contributing
1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## License
MIT License - see LICENSE file for details

================
File: requirements.txt
================
pandas>=1.4.0
tqdm>=4.65.0
